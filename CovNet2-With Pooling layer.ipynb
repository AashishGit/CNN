{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os as os\n",
    "from sklearn.model_selection import train_test_split ####Only for splittinng the dataset, rest is in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/aashish/assignment2')\n",
    "##Chnage the directory on your system to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input data\n",
    "data = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.drop(['label'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = data.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Convert target values to one hot vectors\n",
    "target = pd.get_dummies(label, columns=['label'], drop_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Splite data into training and cross val(testing)\n",
    "x_train,x_cv,y_train,y_cv = train_test_split(train,target,test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reshaping the dataframe to image\n",
    "x_arr = np.array(x_train)\n",
    "x_cv_arr = np.array(x_cv)\n",
    "X = x_arr.reshape(33600,28,28,1)\n",
    "X_cv = x_cv_arr.reshape(8400,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PADDING function\n",
    "def zero_pad(data, pad):\n",
    "    data_pad = np.pad(data,((0,0),(pad,pad),(pad,pad),(0,0)), 'constant')\n",
    "    \n",
    "    return data_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Function to return the indices with max values\n",
    "def idxargmax(a):\n",
    "    idx = np.argmax(a, axis=None)\n",
    "    multi_idx = np.unravel_index(idx, a.shape)\n",
    "    if np.isnan(a[multi_idx]):\n",
    "        nan_count = np.sum(np.isnan(a))\n",
    "        idx = np.argpartition(a, -nan_count-1, axis=None)[-nan_count-1]\n",
    "        idx = np.argsort(a, axis=None)[-nan_count-1]\n",
    "        multi_idx = np.unravel_index(idx, a.shape)\n",
    "    return multi_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Maxpooling function\n",
    "def max_pool(X,f,stride):\n",
    "    (m, w, w, c) = X.shape\n",
    "    pool = np.zeros((m,int((w-f)/stride+1),int((w-f)/stride+1), c))\n",
    "    for e in range(0,m):\n",
    "        for k in range(0,c):\n",
    "            i=0\n",
    "            i = int(i)\n",
    "            while(i<w):\n",
    "                j=0\n",
    "                j = int(j)\n",
    "                while(j<w):\n",
    "                    pool[e,int(i/2),int(j/2),k] = np.max(X[e,i:i+f,j:j+f,k])\n",
    "                    j+=stride\n",
    "                i+=stride\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Softmax cost calculation\n",
    "def softmax_cost(out,y):\n",
    "    eout = np.exp(out, dtype=np.float)  \n",
    "    probs = eout/np.sum(eout, axis = 1)[:,None]\n",
    "    \n",
    "    p = np.sum(np.multiply(y,probs), axis = 1)\n",
    "    prob_label = np.argmax(np.array(probs), axis = 1)    ## taking out the arguments of max values\n",
    "    cost = -np.log(p)    ## (Only data loss. No regularised loss)\n",
    "    \n",
    "    return p,cost,probs,prob_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CNN with convolution, pooling and fully connected layer\n",
    "def conv_net(input_data, Y, W1, b1, theta3,bias3):\n",
    "    \n",
    "    ####Forward propagation\n",
    "    \n",
    "    ## Input shape\n",
    "    m, n_Hi, n_Wi, n_Ci = input_data.shape\n",
    "    \n",
    "    ## no. of filters in layer_1 \n",
    "    l1 = len(W1)    ## All filters for conv layer 1 horizontally stacked in W1  \n",
    "    \n",
    "    (f, f, _) = W1[0].shape   ## Shape of the filter used\n",
    "    pad = 1\n",
    "    ## stride = 1\n",
    "    \n",
    "    ## Convolution layer1 output dimensions\n",
    "    n_H1 = n_Hi+(2*pad)-f + 1\n",
    "    n_W1 = n_Wi+(2*pad)-f + 1\n",
    "   \n",
    "    ## Initializing output image matrices after convolution\n",
    "    conv1 = np.zeros((m,n_H1,n_W1,l1))\n",
    "    \n",
    "    ## Padding the input images\n",
    "    input_pad = zero_pad(input_data,pad)\n",
    "    \n",
    "    ## First convolution layer\n",
    "    for i in range(0,m):                                                  ##looping over the no. of examples\n",
    "        for j in range(0,l1):                                             ##looping over the no. of filters\n",
    "            for x in range(0,n_H1):                                       ##looping over the height of one image \n",
    "                for y in range(0,n_W1):                                   ##looping over the width of one image\n",
    "                    conv1[i,x,y,j] = np.sum(input_pad[i,x:x+f,y:y+f]*W1[j])+b1[j]\n",
    "        conv1[i,:,:,:][conv1[i,:,:,:] <= 0] = 0                           ##relu activation\n",
    "        \n",
    "    \n",
    "    ## Pooling layer after max_pooling filter size of 2x2 and stride 2\n",
    "    pooled_layer = max_pool(conv1, 2, 2)  \n",
    "    \n",
    "    \n",
    "    ## Fully connected layer of neurons\n",
    "    fc1 = pooled_layer.reshape(m,int((n_H1/2)*(n_W1/2)*l1))\n",
    "    \n",
    "    ## Output layer of mx10 activation units\n",
    "    out = np.dot(fc1,theta3) + bias3\n",
    "        \n",
    "    ## Using softmax to get the cost    \n",
    "    p, cost, probs, prob_label = softmax_cost(out, Y)   \n",
    "    \n",
    "    ##Accuracy\n",
    "    acc = []\n",
    "    for i in range(0,len(Y)):\n",
    "        if prob_label[i]==np.argmax(np.array(Y)[i,:]):\n",
    "            acc.append(1)\n",
    "        else:\n",
    "            acc.append(0)\n",
    "\n",
    "    ####### Backpropagation to compute gradients \n",
    "    \n",
    "    d_out = probs - Y\n",
    "\n",
    "    dtheta3 = np.dot(d_out.T, fc1)\n",
    "    dbias3 = np.mean(d_out, axis = 0).reshape(1,10)    \n",
    "\n",
    "    dfc1 = np.dot(theta3,d_out.T)\n",
    "    \n",
    "    dpool = dfc1.T.reshape((m, int(n_H1/2), int(n_W1/2), l1))\n",
    "    dconv1 = np.zeros((m, n_H1, n_W1, l1))\n",
    "    \n",
    "    for k in range(0,m):\n",
    "        for c in range(0,l1):\n",
    "            i=0\n",
    "            while(i<n_H1):\n",
    "                j=0\n",
    "                while(j<n_W1):\n",
    "                    (a,b) = idxargmax(conv1[k,i:i+2,j:j+2,c]) ## Getting indexes of maximum value in the array\n",
    "                    dconv1[k,i+a,j+b,c] = dpool[k,int(i/2),int(j/2),c]\n",
    "                    j+=2\n",
    "                i+=2\n",
    "\n",
    "        dconv1[conv1<=0]=0\n",
    "\n",
    "\n",
    "\n",
    "    dW1_stack = {}\n",
    "    db1_stack = {}\n",
    "    \n",
    "    dW1_stack = np.zeros((m,l1,f,f,1))\n",
    "    db1_stack = np.zeros((m,l1,1))\n",
    "\n",
    "    bW1 = {}\n",
    "    dW1 = np.zeros((l1,f,f,1))\n",
    "    db1 = np.zeros((l1,1))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(0,m):                          ## ## looping through the one batch of 32 examples\n",
    "        for c in range(0,l1):\n",
    "            for x in range(0,n_H1):\n",
    "                for y in range(0,n_W1):\n",
    "                    dW1_stack[i,:,:,c] += dconv1[i,x,y,c]*input_pad[i,x:x+f,y:y+f,:]\n",
    "\n",
    "            db1_stack[i,c] = np.sum(dconv1[i,:,:,c])\n",
    "            \n",
    "        dW1 = np.mean(dW1_stack, axis = 0)\n",
    "        db1 = np.mean(db1_stack, axis = 0)\n",
    "\n",
    "        \n",
    "        \n",
    "    return dW1, db1, dtheta3, dbias3, cost, probs, prob_label, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Optimizer function for updating gradients\n",
    "def optimizer(batch,learning_rate,W1,b1,theta3,bias3):\n",
    "    \n",
    "    ## Slicing train data and labels from batch\n",
    "    X = batch[:,0:-10]\n",
    "    X = X.reshape(len(batch), w, w, l)\n",
    "    Y = batch[:,784:794]\n",
    "    \n",
    "    \n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    ## Initializing gradient matrices \n",
    "    bW1 = {}\n",
    "    dW1 = np.zeros((l1,f,f,1))\n",
    "    db1 = np.zeros((l1,1))\n",
    "    \n",
    "    dtheta3 = np.zeros(theta3.shape)\n",
    "    dbias3 = np.zeros(bias3.shape)\n",
    "    \n",
    "    grads = conv_net(X,Y,W1,b1,theta3,bias3)\n",
    "    [dW1, db1, dtheta3, dbias3, cost_, probs_, prob_label, acc_] = grads\n",
    "    \n",
    "    W1 = W1-learning_rate*(dW1)\n",
    "    b1 = b1-learning_rate*(db1)\n",
    "    theta3 = theta3-learning_rate*(dtheta3.T)\n",
    "    bias3 = bias3-learning_rate*(dbias3)\n",
    "    \n",
    "    batch_cost = np.mean(cost_)\n",
    "    batch_accuracy = sum(acc_)/len(acc_)\n",
    "    print(batch_accuracy)\n",
    "    \n",
    "    return W1, b1, theta3, bias3, batch_cost, acc_, batch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing weights and bias for convolution layer\n",
    "\n",
    "W1 = 0.1*np.random.rand(3,3,3,1)\n",
    "\n",
    "b1 = 0.1*np.random.rand(3,1)\n",
    "\n",
    "## Initializing weights and bias for fully connected layer\n",
    "#def initialize_theta3():\n",
    "theta3 = 0.1*np.random.rand(588,10)\n",
    "#    return theta3\n",
    "\n",
    "bias3 = 0.1*np.random.rand(1,10)\n",
    "\n",
    "## Normalizing input data\n",
    "x_arr -= int(np.mean(x_arr))\n",
    "x_arr = x_arr.astype(float)\n",
    "x_arr /= int(np.std(x_arr))\n",
    "\n",
    "train_data = np.hstack((x_arr,np.array(y_train)))     ## horizontally stacking the features and labels \n",
    "\n",
    "## training the model on only 320 examples images due to heavy computation issue\n",
    "t = train_data[0:320]      \n",
    "\n",
    "## Normalizing cross-validation data\n",
    "x_cv_arr -= int(np.mean(x_cv_arr))\n",
    "x_cv_arr = x_cv_arr.astype(float)\n",
    "x_cv_arr /= int(np.std(x_cv_arr))\n",
    "\n",
    "cv_data = np.hstack((x_cv_arr,np.array(y_cv)))\n",
    "\n",
    " ## cross-validating the model on only 100 examples images due to heavy computation issue   \n",
    "test_data = x_cv_arr[0:100]   \n",
    "\n",
    "Y_cv = np.array(y_cv)[0:100]\n",
    "\n",
    "np.random.shuffle(train_data)\n",
    "\n",
    "## Assigning hyperparameter values\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "num_images = len(t)   ##Number of the input training examples\n",
    "w = 28\n",
    "l = 1\n",
    "l1 = len(W1)    ## no. opf filters in W1      \n",
    "f = len(W1[0])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_init(train_data,W1,b1,theta3,bias3):\n",
    "    cost = []\n",
    "    accuracy = []\n",
    "    for epoch in range(0, num_epochs):\n",
    "        batches = [train_data[k:k + batch_size] for k in range(0, len(train_data), batch_size)]\n",
    "        x=0\n",
    "        i = 1\n",
    "        for batch in batches:\n",
    "            \n",
    "            output = optimizer(batch,learning_rate,W1,b1,theta3,bias3)\n",
    "            [W1, b1, theta3, bias3, batch_cost,acc_,batch_acc] = output\n",
    "                        \n",
    "            cost.append(batch_cost)\n",
    "            accuracy.append(batch_acc)\n",
    "\n",
    "            print ('ep:%d, batch_num = %f, batch_cost = %f, batch_acc = %f' %(epoch,i,batch_cost,batch_acc)) \n",
    "            i+=1\n",
    "    return W1,b1,theta3,bias3,cost,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1875\n",
      "ep:0, batch_num = 1.000000, batch_cost = 2.339122, batch_acc = 0.187500\n",
      "0.1875\n",
      "ep:0, batch_num = 2.000000, batch_cost = 2.326078, batch_acc = 0.187500\n",
      "0.3125\n",
      "ep:0, batch_num = 3.000000, batch_cost = 2.173453, batch_acc = 0.312500\n",
      "0.25\n",
      "ep:0, batch_num = 4.000000, batch_cost = 2.182976, batch_acc = 0.250000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-f889e2020ea1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mW1_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb1_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtheta3_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbias3_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcost_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtheta3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbias3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cost Function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of iterations'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-8208e7828f33>\u001b[0m in \u001b[0;36mmain_init\u001b[0;34m(train_data, W1, b1, theta3, bias3)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtheta3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbias3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_cost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_acc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-42dacc852f6d>\u001b[0m in \u001b[0;36moptimizer\u001b[0;34m(batch, learning_rate, W1, b1, theta3, bias3)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdbias3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtheta3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbias3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mdW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtheta3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbias3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-ada130bd5dae>\u001b[0m in \u001b[0;36mconv_net\u001b[0;34m(input_data, Y, W1, b1, theta3, bias3)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_H1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_W1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                     \u001b[0mdW1_stack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdconv1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_pad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mdb1_stack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdconv1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "W1_t,b1_t,theta3_t,bias3_t,cost_t,accuracy_t = main_init(t,W1,b1,theta3,bias3)\n",
    "plt.plot(cost_t, linewidth = 2)\n",
    "plt.ylabel('Cost Function')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_data, Y_cv, W1, b1, theta3, bias3, cost_t):\n",
    "    \n",
    "    ######## Forward Propagation \n",
    "    \n",
    "    test_data = test_data.reshape(len(test_data),w,w,l)\n",
    "    \n",
    "    ## test shape\n",
    "    m, n_Hi, n_Wi, n_Ci = test_data.shape\n",
    "    \n",
    "    ## no. of filters in layer_1 \n",
    "    l1 = len(W1)    ## All filters for conv layer 1 horizontally stacked in W1  \n",
    "    \n",
    "    (f, f, _) = W1[0].shape   ## Shape of the filter used\n",
    "    pad = 1\n",
    "    ## stride = 1\n",
    "    \n",
    "    ## Convolution layer1 output dimensions\n",
    "    n_H1 = n_Hi+(2*pad)-f + 1\n",
    "    n_W1 = n_Wi+(2*pad)-f + 1\n",
    "   \n",
    "    \n",
    "    ## Initializing output image matrices after convolution\n",
    "    conv1 = np.zeros((m,n_H1,n_W1,l1))\n",
    "    \n",
    "    ## Padding the test images\n",
    "    test_pad = zero_pad(test_data,pad)\n",
    "    \n",
    "    ## First convolution layer\n",
    "    for i in range(0,m):                                                  ##looping over the no. of examples\n",
    "        for j in range(0,l1):                                             ##looping over the no. of filters\n",
    "            for x in range(0,n_H1):                                       ##looping over the height of one image \n",
    "                for y in range(0,n_W1):                                   ##looping over the width of one image\n",
    "                    conv1[i,x,y,j] = np.sum(test_pad[i,x:x+f,y:y+f]*W1[j])+b1[j]\n",
    "        conv1[i,:,:,:][conv1[i,:,:,:] <= 0] = 0                           ##relu activation\n",
    "        \n",
    "    \n",
    "    ## Pooling layer after max_pooling filter size of 2x2 and stride 2\n",
    "    pooled_layer = max_pool(conv1, 2, 2)  \n",
    "    \n",
    "    \n",
    "    ## Fully connected layer of neurons\n",
    "    fc1 = pooled_layer.reshape(m,int((n_H1/2)*(n_W1/2)*l1))\n",
    "    \n",
    "    ## Output layer of mx10 activation units\n",
    "    out_t = np.dot(fc1,theta3) + bias3\n",
    "        \n",
    "    ## Using softmax to get the cost    \n",
    "    p_pred, cost_pred, probs_pred, prob_label_pred = softmax_cost(out_t, Y_cv)  \n",
    "    \n",
    "    cv_acc = []\n",
    "    for i in range(0,len(Y_cv)):\n",
    "        if prob_label_pred[i]==np.argmax(np.array(Y_cv)[i,:]):\n",
    "            cv_acc.append(1)\n",
    "        else:\n",
    "            cv_acc.append(0)\n",
    "            \n",
    "    cv_accuracy = sum(cv_acc)/len(cv_acc)\n",
    "    print ('cv_acc = %f' %(cv_accuracy))\n",
    "    \n",
    "    \n",
    "    return cost_pred, probs_pred, prob_label_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_pred_tst, probs_pred_tst, prob_label_pred_tst = predict(test_data, Y_cv, W1_t, b1_t, theta3_t, bias3_t, cost_t)\n",
    "plt.plot(cost_pred_tst, linewidth = 2)\n",
    "plt.ylabel('Cost Function')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
